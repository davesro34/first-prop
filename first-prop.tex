\documentclass[12pt]{article}
\setlength{\textheight}{210mm}
\addtolength{\topmargin}{-18mm}
\setlength{\textwidth}{165mm}
\setlength{\oddsidemargin}{5mm}
\usepackage{caption}
\usepackage{graphicx, subcaption, amsfonts}
\graphicspath{ {./figs/} }
\pagestyle{plain}
\begin{document}
\title{Manifold Learning to Enable Equation-Free Modeling of Chemical Engineering Systems}
\author{\LARGE Proposed by David Sroczynski\vspace{3mm}\\\Large under the supervision of\vspace{3mm}\\\LARGE Professor Yannis Kevrekidis}
\date{11/04/2015}
\maketitle

\clearpage
\tableofcontents
\clearpage
\section{Introduction}
Virtually every field of science and engineering uses modeling to expand and elucidate the information available from experiments. These models range from simple one variable ODEs for a CSTR to computational fluid dynamics code. Experimental data is inherently messy; a well-developed model can bring clarity to complicated systems. Models have the additional advantage that they can be initialized as often as desired in whatever state is desired, while experiments can be challenging or expensive to initiate. The obvious caveats are that a model must be accurate enough to capture the "important" information while being compact enough to be solved in a reasonable timeframe.  \vspace{1mm}

The field of dimensionality reduction explores the question of what information is "important". The concept is very old; for example, we learned early on that in the analysis of a chemical reaction, the important variables often include temperature and concentrations. The reaction may involve collisions between individual atoms, but we only need to know certain things about the average behavior to predict the results that we are interested in. However, as we encounter new systems (which are often more complex), we desire more systematic ways to extract these important variables. The most ubiquitous method is principal components analysis (PCA), which can be traced back to as early as 1901 \cite{Pearson1901}, but was developed across many fields under many names over much of the $20^{th}$ century \cite{Hotelling1933}. PCA can be thought of as an extension of the line-of-best-fit method to higher dimensions: if the data is viewed as a cloud of points in some high dimensional space, PCA determines which directions characterize the highest variability in the data. Projecting the data onto only these directions results in a reduced representation of the data that still captures most of the variability. One drawback to PCA is that it only captures linear relationships (consider the case of a line-of-best-fit vs. a nonlinear curve fit). The field of nonlinear dimensionality reduction has grown rapidly in recent years with the introduction of methods like kernel PCA \cite{Scholkopf1998}, local linear embedding \cite{Roweis2000}, Isomap \cite{Tenenbaum2000}, and diffusion maps \cite{Coifman2005} \cite{Coifman2005a} \cite{Coifman2006}.  \vspace{1mm}

Dimensionality reduction methods have the potential to greatly improve models in terms of clarity and speed by reducing the model to the lower-dimensional representations that are easier to visualize and faster to compute. We are interested in using diffusion maps to improve model performance in molecular simulation and other relevant chemical engineering systems. Diffusion maps has recently shown promise in data analysis for models in transport \cite{Sonday2009}, chemical kinetics \cite{Chiavazzo2014}, and molecular dynamics \cite{Ferguson2010} \cite{Nedialkova2014} \cite{Kim2015}. We intend to expand on this work in the context of the "equation-free" (EF) framework, which uses short simulations of detailed models as the basis for projection in a coarse model (whose variables can be determined by diffusion maps). This idea of data analysis for the purpose of computation includes the idea that diffusion maps can aid in the development of a model or the characterization of dynamical systems by biasing where to next take data to gain "important" information. This in particular has applications to molecular simulation, where the characterization of rare events can be extremely expensive; methods to speed up these computations often require a good coarse variable to bias the simulation.  \vspace{1mm}

This document will first explain the EF framework and then discuss diffusion maps and some of the challenges in its implementation. We will describe current work being done on a data-centric application in developmental dynamics, and then we will conclude with systems of interest in future work.  \vspace{1mm}

\section{Equation-Free Modeling}

EF modeling refers to the framework of using exisiting detailed microscopic simulators in a black-box manner to enable solutions to macroscopic tasks that are intractable for the microscopic simulator in its original formulation \cite{Siettos2003} \cite{Kevrekidis2003} \cite{Kevrekidis2004}. Consider such a simulator (e.g., MD or CFD code) which performs detailed time-stepping on the microscopic level given certain initial conditions and parameters. EF modeling requires the existence of an appropriate coarse (lower dimensional) description, where appropriate in this context means that it should capture the "important" information. We want the detailed simulation to track along some lower dimensional manifold (which can be represented by our coarse description) embedded in the detailed, high dimensional space. Simulations that are initialized away from this manifold should very quickly move to the manifold in the spirit of chemical kinetics systems that follow the quasi-steady-state approximation. \vspace{1mm}

EF modeling also requires that we be able to move between coarse (low dimensional) and fine (high dimensional) descriptions through some reasonable tractable operators. A restriction operator converts a fine description to a coarse description; for example, we might restrict the speeds of each molecule in MD simulation to the coarse variable temperature. Lifting operators, which do the reverse, often have the added complication that there are many fine descriptions that correspond to the same coarse description, so some care needs to be taken in systematically choosing a fine description. In many cases, however, even if the fine simulation is initialized poorly, it will quickly move back to the appropriate manifold. This is typically referred to as healing. \vspace{1mm}

Given the detailed simulator, the appropriate coarse description, and the two operators, the most straightforward application of the EF framework is coarse projective integration (CPI), where we step the coarse variables forward in time based on properly initialized fine simulations. For each integration step, the procedure is as follows:  \vspace{1mm}

\begin{enumerate}
\item Use the lifting operator to determine initial conditions for the fine simulation based on the current values of the coarse variables.
\item Run the fine simulation for a short healing period to bring it back to the appropriate manifold.
\item Continue to run the fine simulation until sufficient information is captured about about the progression of the coarse variables.
\item Project the coarse variables forward in time using forward Euler integration or any other integration scheme.
\end{enumerate}

If the simulation data actually lie on or near a manifold characterized by the coarse variables, coarse projective integration offers vast speed-ups compared to straight integration of the fine simulation. This framework of using fine simulations to determine the coarse behavior also has applications in stability/bifurcation analysis \cite{Theodoropoulos} \cite{Gear2002} and the exploration of potential surfaces \cite{Frewen2009}. We are interested in applying this approach to systems where the coarse variables are not known \textit{a priori} but must be determined through dimensionality reduction.

\section{Diffusion Maps}

Diffusion maps is a method of analyzing the geometry of data and discovering lower-dimensional manifolds that the data approximates. The algorithm is designed to approximate the continuous Laplace-Beltrami operator (which has been shown to provide good parametrizations of nonlinear manifolds) to discrete data. Suppose you have \textit{m} data points in \textit{n}-dimensional space represented by $y_1$,...,$y_\textit{m}$. The algorithm first constructs a weight matrix such that\\
\[
W_{ij}=exp(-\frac{||y_i-y_j||^2}{\sigma ^2})
\]
where $|| \bullet ||$ is an appropriate norm or distance metric between to data points and $\sigma$ is a characteristic distance such that points are considered close. Diffusion maps treats distances smaller than $\sigma$ as important but treats distances much longer than $\sigma$ as meaningless. The matrix is then made row-stochastic by dividing each row by its sum so that the rows sum to $1$. This gives $W$ the interpretation of a Markov matrix such that the elements represent transition probabilities from one data point to another. Variations to the algorithm exist which can, among other things, account for variations in sampling density.\vspace{1mm}

The eigendecomposition of W yields eigenvalues $\lambda _0$,...,$\lambda_{m-1}$ and eigenvectors $\phi _0$,...,$\phi_{m-1}$. Due to row-stochasticity, the first eigenvector $\phi_0$ is a trivial constant vector with $\lambda_0 = 1$. The other eigenvectors provide a new coordinate system such that the $k^{th}$ component of $y_i$ is given by the $i^{th}$ component of $\phi_k$, scaled by $\lambda_k$. In this new coordinate system, distance between two points is referred to as the diffusion distance. This diffusion distance represents moving from one point to another by diffusion, where you can only move to nearby points based on the probabilities in the weight matrix. Since diffusion distance is based on moving from point to point, it approximates the distance along the manifold that the points lie on. If there is a large spectral gap, meaning that some eigenvalues are significantly larger than others, then the diffusion distance can be accurately approximated using only coordinates with the largest eigenvalues.  The number of these large eigenvalues gives information about the true dimensionality of the manifold.




\section{Application to Data Fusion: \textit{Drosophila} embryos}



\section{Systems of Interest}



\section{Conclusions}

\bibliographystyle{abbrv}
\bibliography{../first-prop}
\end{document}