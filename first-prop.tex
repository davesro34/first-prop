\documentclass[12pt]{article}
\setlength{\textheight}{210mm}
\addtolength{\topmargin}{-18mm}
\setlength{\textwidth}{165mm}
\setlength{\oddsidemargin}{5mm}
\usepackage{caption}
\usepackage{graphicx, subcaption, amsfonts, amsmath}
\graphicspath{ {./figs/} }
\pagestyle{plain}
\begin{document}
\title{Manifold Learning to Enable Equation-Free Modeling of Chemical Engineering Systems}
\author{\LARGE Proposed by David Sroczynski\vspace{3mm}\\\Large under the supervision of\vspace{3mm}\\\LARGE Professor Yannis Kevrekidis}
\date{11/04/2015}
\maketitle
\thispagestyle{empty}
\clearpage
\tableofcontents
\thispagestyle{empty}
\clearpage
\pagenumbering{arabic}
\section{Introduction}
Virtually every field of science and engineering uses modeling to expand and elucidate the information available from experiments. These models range from simple one variable ODEs for a CSTR to computational fluid dynamics code. Experimental data is inherently messy; a well-developed model can bring clarity to complicated systems. Models have the additional advantage that they can be initialized as often as desired in whatever state is desired, while experiments can be challenging or expensive to initiate. The obvious caveats are that a model must be accurate enough to capture the "important" information while being compact enough to be solved in a reasonable timeframe.  \vspace{1mm}

The field of dimensionality reduction explores the question of what information is "important". The concept is very old; for example, we learned early on that in the analysis of a chemical reaction, the important variables often include temperature and concentrations. The reaction may involve collisions between individual atoms, but we only need to know certain things about the average behavior to predict the results that we are interested in. However, as we encounter new systems (which are often more complex), we desire more systematic ways to extract these important variables. The most ubiquitous method is principal components analysis (PCA), which can be traced back to as early as 1901 \cite{Pearson1901}, but was developed across many fields under many names over much of the $20^{th}$ century \cite{Hotelling1933}. PCA can be thought of as an extension of the line-of-best-fit method to higher dimensions: if the data is viewed as a cloud of points in some high dimensional space, PCA determines which directions characterize the highest variability in the data. Projecting the data onto only these directions results in a reduced representation of the data that still captures most of the variability. One drawback to PCA is that it only captures linear relationships (consider the case of a line-of-best-fit vs. a nonlinear curve fit). The field of nonlinear dimensionality reduction has grown rapidly in recent years with the introduction of methods like kernel PCA \cite{Scholkopf1998}, local linear embedding \cite{Roweis2000}, Isomap \cite{Tenenbaum2000}, and diffusion maps \cite{Coifman2005} \cite{Coifman2005a} \cite{Coifman2006}.  \vspace{1mm}

Dimensionality reduction methods have the potential to greatly improve models in terms of clarity and speed by reducing the model to the lower-dimensional representations that are easier to visualize and faster to compute. We are interested in using diffusion maps to improve model performance in molecular simulation and other relevant chemical engineering systems. Diffusion maps has recently shown promise in data analysis for models in transport \cite{Sonday2009}, chemical kinetics \cite{Chiavazzo2014}, and molecular dynamics \cite{Ferguson2010} \cite{Nedialkova2014} \cite{Kim2015}. We intend to expand on this work in the context of the "equation-free" (EF) framework, which uses short simulations of detailed models as the basis for projection in a coarse model (whose variables can be determined by diffusion maps). This idea of data analysis for the purpose of computation includes the idea that diffusion maps can aid in the development of a model or the characterization of dynamical systems by biasing where to next take data to gain "important" information. This in particular has applications to molecular simulation, where the characterization of rare events can be extremely expensive; methods to speed up these computations often require a good coarse variable to bias the simulation.  \vspace{1mm}

This document will first explain the EF framework and then discuss diffusion maps and some of the challenges in its implementation. We will describe current work being done on a data-centric application in developmental dynamics, and then we will conclude with systems of interest in future work.  \vspace{1mm}

\section{Equation-Free Modeling}

EF modeling refers to the framework of using exisiting detailed microscopic simulators in a black-box manner to enable solutions to macroscopic tasks that are intractable for the microscopic simulator in its original formulation \cite{Siettos2003} \cite{Kevrekidis2003} \cite{Kevrekidis2004}. Consider such a simulator (e.g., MD or CFD code) which performs detailed time-stepping on the microscopic level given certain initial conditions and parameters. EF modeling requires the existence of an appropriate coarse (lower dimensional) description, where appropriate in this context means that it should capture the "important" information. We want the detailed simulation to track along some lower dimensional manifold (which can be represented by our coarse description) embedded in the detailed, high dimensional space. Simulations that are initialized away from this manifold should very quickly move to the manifold in the spirit of chemical kinetics systems that follow the quasi-steady-state approximation. \vspace{1mm}

EF modeling also requires that we be able to move between coarse (low dimensional) and fine (high dimensional) descriptions through some reasonable tractable operators. A restriction operator converts a fine description to a coarse description; for example, we might restrict the speeds of each molecule in MD simulation to the coarse variable temperature. Lifting operators, which do the reverse, often have the added complication that there are many fine descriptions that correspond to the same coarse description, so some care needs to be taken in systematically choosing a fine description. In many cases, however, even if the fine simulation is initialized poorly, it will quickly move back to the appropriate manifold. This is typically referred to as healing. \vspace{1mm}

Given the detailed simulator, the appropriate coarse description, and the two operators, the most straightforward application of the EF framework is coarse projective integration (CPI), where we step the coarse variables forward in time based on properly initialized fine simulations. For each integration step, the procedure is as follows:  \vspace{1mm}

\begin{enumerate}
\item Use the lifting operator to determine initial conditions for the fine simulation based on the current values of the coarse variables.
\item Run the fine simulation for a short healing period to bring it back to the appropriate manifold.
\item Continue to run the fine simulation until sufficient information is captured about about the progression of the coarse variables.
\item Project the coarse variables forward in time using forward Euler integration or any other integration scheme.
\end{enumerate}

If the simulation data actually lie on or near a manifold characterized by the coarse variables, coarse projective integration offers vast speed-ups compared to straight integration of the fine simulation. This framework of using fine simulations to determine the coarse behavior also has applications in stability/bifurcation analysis \cite{Theodoropoulos2000} \cite{Gear2002} and the exploration of potential surfaces \cite{Frewen2009}. We are interested in applying this approach to systems where the coarse variables are not known \textit{a priori} but must be determined through dimensionality reduction.

\section{Diffusion Maps}

\subsection{Overview} \label{DMO}

Diffusion maps is a method of analyzing the geometry of data and discovering lower-dimensional manifolds that the data approximates. The algorithm is designed to approximate the continuous Laplace-Beltrami operator (which has been shown to provide good parametrizations of nonlinear manifolds) to discrete data. Suppose you have \textit{m} data points in \textit{n}-dimensional space represented by $y_1$,...,$y_\textit{m}$. The algorithm first constructs a weight matrix such that\\
\[
W_{ij}=exp(-\frac{||y_i-y_j||^2}{\sigma ^2})
\]
where $|| \bullet ||$ is an appropriate norm or distance metric between to data points and $\sigma$ is a characteristic distance such that points are considered close. Diffusion maps treats distances smaller than $\sigma$ as important but treats distances much longer than $\sigma$ as meaningless. The matrix is then made row-stochastic by dividing each row by its sum so that the rows sum to $1$. This gives $W$ the interpretation of a Markov matrix such that the elements represent transition probabilities from one data point to another. Variations to the algorithm exist which can, among other things, account for variations in sampling density.\vspace{1mm}

The eigendecomposition of W yields eigenvalues $\lambda _0$,...,$\lambda_{m-1}$ and eigenvectors $\phi _0$,...,$\phi_{m-1}$. Due to row-stochasticity, the first eigenvector $\phi_0$ is a trivial constant vector with $\lambda_0 = 1$. The other eigenvectors provide a new coordinate system such that the $k^{th}$ component of $y_i$ is given by the $i^{th}$ component of $\phi_k$, scaled by $\lambda_k$. In this new coordinate system, distance between two points is referred to as the diffusion distance. This diffusion distance represents moving from one point to another by diffusion, where you can only move to nearby points based on the probabilities in the weight matrix. Since diffusion distance is based on moving from point to point, it approximates the distance along the manifold that the points lie on. If there is a large spectral gap, meaning that some eigenvalues are significantly larger than others, then the diffusion distance can be accurately approximated using only coordinates with the largest eigenvalues.  The number of these large eigenvalues gives information about the true dimensionality of the manifold.

\subsection{Choosing a distance metric and kernel scale}

Since the diffusion maps algorithm is based entirely on the distances between points, choosing an approriate distance metric is critical. Assuming that each data point is represented by a vector such that each element represents one variable that describes the state (e.g., concentration of one chemical species, signal intensity at some point in space, etc.), then the obvious solution is simply the Euclidian distance between the two points described by the two data vectors. Unfortunately, in many instances, straightforward application of the Euclidean distance is not an informative measure of similarity. Here is a listing of some alternate approaches, and a short explanation of when they might be useful:

\begin{enumerate}
\item \textbf{Data preprocessing.}  This is a general term for cleaning up data so that the Euclidean distance becomes more informative. This can include actions like standardization of variables which span disparate scales, blurring of spatially organized data to remove uninformative small-scale structure, and contrast boosting or other image enhancement.
\item \textbf{Feature extraction.} In certain cases, \textit{a priori} knowledge about the system can justify the selection of informative features. These functions of the original data can include ratios of quantities, times to achieve certain benchmarks, or quantities of distributions. This is essentially an initial step of dimensionality reduction before allowing the formal algorithm to finish the job. Unfortunately, this \textit{a priori} knowledge is often unavailable for new systems.
\item \textbf{Mahalanobis distance.}  This metric is primarily useful in stochastic systems where movement along the slow manifold is obscured by independent white noise. Based on the covariance of the data, the Mahalanobis distance ignores directions which do not indicate meaningful dissimilarity. Given column vector observations $y_i$ and $y_j$ from a distribution with covariance $C$, the Mahalanobis distance is given by
\[
||y_i-y_j||^2_M = (y_i-y_j)^TC^{-1}(y_i-y_j)
\]
The covariance matrix typically must be estimated from the data, and often the estimation is not full-rank so a pseudoinverse is required.
\item \textbf{Earth mover's distance}. Also known as the Wasserstein metric, this is a measure of distance between two probability ditributions, or more practically, between two normalized histograms. The earth mover's distance quantifies how much work is required to change one histogram into another. The choice to use the histogram is a form of feature extraction that is useful when the individual quantities are less informative than their distribution, as is often the case in applications such as molecular simulation.
\item \textbf{Graph metrics} A graph is a representation of data that defines nodes (which can be people, websites, locations, etc.) and edges (connections) between nodes. These edges can be either weighted or unweighted, and either directed or undirected. Methods for graph similarity include edit distance, maximal common subgraph, and graph kernels. These methods are often computationally intractable, but methods exist for their approximation.
\end{enumerate}

Once the distance metric is chosen, the kernal scale $\sigma$ must also be chosen. This often requires some trial and error, but a good initial guess is to use some fraction (e.g., $\frac{1}/{2}$) of the median pairwise distance between data points. An alternate approach is to use the maximum distance to some number of nearest neighbors, averaged over each data point.

\subsection{Synchronization of data}

Another issue that can occur in the application of diffusion maps is when data points have some degree of freedom among some group that obscures the dynamics. One example is dynamics under periodic boundary conditions where we often want to factor out the translation. Other examples include 2D images that can be arbitrarily rotated, or molecular simulations of a complex molecule that can be arbitrarily shifted and rotated in 3D space. A common way to solve this problem is to align each images based on some template, but when the dynamics are complicated, it is not always obvious how to choose a template without \textit{a priori} knowledge, and pairwise alignment between a data point and a template can be noisy. The eigenvector alignment method solves this problem by considering each data point as the template for each of the other points and finding globally optimal alignments for each image. \vspace{1mm}

The algorithm requires that the transformation between members of the group be represented by some operator $g_{ij}$ such that the operator satisfies the triplet consistency relation:
\[
g_{ik} = g_{ij}g_{jk}
\]
In the case of 1D periodic boundary conditions, we can consider the dynamics to be taking place on a ring, and then the operator $g_{ij}$ becomes $exp(-\theta_{ij})$, where $\theta_{ij}$ is the shift that takes data point $j$ to data point $i$. For general rotations, $g_{ij}$ becomes the rotation matrix $R_{ij}$, which can be $2$ by $2$ for rotations in the plane or $3$ by $3$ for rotations in 3d space. Once this operator is defined, we constrict the matrix $G$ such that each the $ij^{th}$ element (or $ij^{th}$  block) of $G$ is the operator $g_{ij}$. The top eigenvector (or eigenvectors in the block matrix case) gives the operators $g_i$ which transformed the optimal representation into data point $i$. This representation has degrees of freedom within the group (in the case of rotation within the plane, the solutions could be arbitrarily rotated within the plane), but it is optimized based on all pairwise comparisons. \vspace{1mm}

An extension called vector diffusion maps exists which simultaneously aligns the images while performing diffusion maps. VDM is useful in cases where the dynamics are dramatic, and pairwise alignments between dynamically different data points become meaningless. VDM ignores these alignments and simultaneously provides global alignment information and dimensionality reduction.

\subsection{Lifting and Restriction Operators}

Various choices exist for lifting and restriction operators, which are essentially interpolation/curve fitting schemes. Several choices are summarized here; more detailed explanations and analysis can be found in \cite{Chiavazzo2014}. The problem statement is as follows: given a set of detailed descriptions $y_1$,...,$y_\textit{m}$, along with a set of corresponding coarse descriptions $\phi_1$,...,$\phi_\textit{m}$ from diffusion maps, how do we assign a new coarse description $\phi$ to a new detailed state $y$ and vice versa.

\begin{enumerate}
\item \textbf{Nearest Neighbor.} The simplest approach is to take some an average of some number of nearest neighbors, possibly with some weighting based on distance to each neighbor. A slightly more sophisticated approach is to fit functional (often linear) relationships between the coarse and fine variables based on just the nearest neighbors.
\item \textbf{Nystr\"om Extension.} This is one of the most common methods for finding diffusion maps coordinates for a new detailed state. The new coordinate is essentially calculated as a weighted sum of all of the previous coordinates with weights based on the diffusion kernel. The equation for the $\alpha^{th}$ component of the new coarse description is given by
\[
\phi_{\alpha} = \frac{1}{\lambda_\alpha}*\frac{\sum_{i=1}^m \phi_{i,\alpha}*exp(-\frac{||y-y_i||^2}{\sigma ^2})}{\sum_{i=1}^m exp(-\frac{||y-y_i||^2}{\sigma ^2})}
\]
The presence of $\frac{1}{\lambda_\alpha}$ ensures that applying the procedure to the existing points gives the same coordinates as the original diffusion maps algorithm; this can be verified by considering the eigenvalue problem in section \ref{DMO}. \vspace{1mm}

In its typical formulation, the Nystr\"om extension is only a restriction operator, but it is possible to consider a lifting self-consistency problem of finding a new data point that would give a specific coarse description from Nystr\"om. While this has not been well-studied, it would likely involve opimization of an initial guess based on another method.
\item \textbf{Radial Basis Functions}. This method can be implemented based on any function that depends only on the distance between two input points. Both lifting and restriction operators can be expressed on a sum over $nn$ nearest neighbors:
\[
\phi_{\alpha} =\sum_{i=1}^{nn} \beta_{i,\alpha}*f(||y-y_i||)
\]
The coefficients $\beta$ can be determined using linear algebra based on the $nn$ fitting points. The Nystr\"om extension can be thought of as a modified specific case of RBF where the radial function $f$ is the diffusion kernel and each $\beta$ involves $\lambda_\alpha$ as well as the normalization constant (which depends on $y$, unlike in RBF). Other common basis functions include simple powers of the distance.
\item \textbf{Kriging.} Also known as Gaussian Process Regression, this method views the functional relationship as a realization of a stochastic Gaussian process based on the assumption that points that are nearby in space are statistically correlated. The first step is to fit (based on the data) a model for the semivariogram
\[
\gamma(y_i,y_j)=var(\phi(y_i)-\phi(y_j))
\]
where var($\bullet$) is the variance. The semivariogram is a measure of how different the coarse descriptions at $y_i$ and $y_j$ are likely to be; it often taken to be simply a function of the distance, although modifications exist to account for variation due to location and orientation in fine space. Depending on the assumptions, the Kriging equations can be slightly different: good discussions can be found in \cite{Press2007} and \cite{Isaaks1989}.
\item \textbf{Laplacian Pyramids.} This is a multiscale method which finds $\phi_\alpha$ as a sum of fits on smaller and smaller scales. Assuming a given kernel $k$ (typically the normalized diffusion kernel from Nystr\"om using smaller and smaller $\sigma$), the first fit is found by
\[
s^{(0)}_\alpha=\sum_{i=1}^{nn} k^{(0)}(y_i,y)*\phi_{i,\alpha}
\]
Subsequent fits are found by reducing the kernel scale and fitting to the residual:
\[
s^{(l)}_\alpha=\sum_{i=1}^{nn} \left( k^{(l)}(y_i,y)*(\phi_{i,\alpha}-\sum_{j-0}^{l-1}s^{(j)}_\alpha )\right)
\]
After evaluating to an appropriate scale $l_{max}$, the final solution is given by
\[
\phi_\alpha=\sum_{l=0}^{l_{max}}s^{(l)}_\alpha
\]
Alternative multiscale methods exist, such as geometric harmonics.
\end{enumerate}

\subsection{Extending outside of the support}
There is a nontrivial issue that arises when attempting to do calculations with diffusion maps coordinates that extend outside the support of the training data. The diffusion maps eigenproblem is an approximation of the Laplace-Beltrami operator on an underlying manifold. In the typical framework, diffusion only takes place between the data points, which implies no flux at the boundary. In the continuous case, the eigenfunctions involve cosines, so the slope at the boundary is 0. This means that attempting to model the behavior of diffusion maps coordinates outside the boundary is ill-posed; the diffusion maps coordinate is not changing at the boundary, and finding a fine state with a diffusion maps coordinate greater than the maximum is not possible for linear fit. Even in the discrete approximation where the slope doesn't go fully to 0, extension past the boundary is extremely error prone. \vspace{1mm}

It is desirable to eliminate the zero slope at the boundary by modifying the eigenproblem to approximate boundary conditions with a specified flux. While some work has been done in this area, it remains more of an art than a science. It is necessary to first identify boundary points in each coarse direction. Row-stochasticity is eliminated for these points to simulate diffusion into the system at one boundary and out of the system at the other boundary. This allows much better modeling and extrapolation near the boundary.

\section{Application to Data Fusion: \textit{Drosophila} embryos}

\subsection{Data description and motivating problem}

One application of diffusion maps is in the study of developmental dynamics. This application is not within the EF framework and is more focused on the data itself rather than using the data for calculations, but it does showcase many of the relevant tools in data analysis that form the backbone of EF modeling. The specific case is the development of 	\textit{Drosophila} embryos, where researchers are interested in tracking the embryo structure (as represented by the locations of cell nuclei) as well as the distribution of various relevent proteins. There are two relevant types of data sets:

\begin{enumerate}
\item A live movie set consists of images every 30 seconds of the same embryo over some range of its development time. These images have nuclei labeled by Histone-RFP, and also have associated time stamps.
\item Fixed snapshot data sets consist of one image from each of a set of embryo. These embryo have three channels; they are stained different colors for the nuclei as well as two proteins, Twist and dpERK. These images do not have time stamps, and they may also be arbitrarily rotated in the plane.
\end{enumerate}

The motivating problem is to construct an average developmental trajectory that contains information about all three channels as well as approximate timings. Prior work in the group has shown that the vector diffusion maps algorithm can very accurately reconstruct the ordering of the live movies even when they are scrambled and arbitrarily rotated. It has also been shown that VDM can also correctly rotate a data set of fixed snapshots, and the ordering is reasonably well consistent with manual ordering by an expert. The next step is to fuse these data sets together to produce a trajectory with color and time.

\subsection{Challenges in data preprocessing}

Each image consists of $100$ by $100$ pixels.  The live movies have just one channel value at each pixel, while the fixed snapshots have three. Each image can thus be viewed as a vector of length $100,000$ (or $300,000$ for fixed snapshots), and a simple first choice of metric is the Euclidean distance between two such vectors. Unfortunately, some significant work needs to be done to make that Euclidean distance informative. The imaging process can produce various image artifacts that are not relevent to the developmental dynamics, especially for the fixed snapshots which are all different embryo. Images need to be centered in the field of view and resized to occupy a consistent portion of the frame. Contrast limited adaptive histogram equalization (CLAHE) is used to normalize the images. We recently found that boosting the contrast significantly improves the performance, probably because whether or not a pixel is occupied is more important than the specific brightness level. Finally, we blur the images using a Gaussian filter to remove small scale structure that distorts the comparison. Variability between embryo means that features will not be in the exact same place; blurring allows such features to still be properly compared.

\subsection{Synchronization of rotations and movie times}

While the embryos in the live movies stay at the same orientation throughout development, the embryos in the fixed snapshots can be in any arbitrary orientation within the plane. While manual registration is possible, it is extremely tedious. The eigenvector alignment method is capable of registering the fixed snapshots with respect to each other in just seconds of computational time. As previously discussed, the final orientation is arbitrary, so there is a final step of rotating the snapshots to align with the live movies in order to do any comparison. The eigenvector alignment method also provides a framework for aligning the different live movies and the snapshots with respect to each other, but with only 7 live movies and 1 data set of snapshots, the gains over manual registration are minimal. \vspace{1mm}

Another synchronization of interest is that of movie times.  When diffusion maps is run on multiple live movies simultaneously, we find that within each live movie there is a good monotonic relationship between time and the first diffusion maps coordinate. However, these different functional relationships are offset from each other. We believe that while some of this error is due to noise and embryo variability, issues with experimental design could be causing some of the discrepancy. For example, lack of precision in the fertilization time could mean that the times for one embryo are shifted in relation to another. Differences in ambient temperature or other environmental factors could cause one embryo to develop at a faster rate. We are interesting in finding factors by which to scale and shift the movie times so that they optimally align with each other. If we consider operations of the form
\[
t_j = a_{ij}*t_i+b_{ij}
\]
then the eigenvector alignment method can be used by considering the matrix operator
\[
g_{ij}=
\begin{bmatrix}
1 && 0\\
a_{ij} && b_{ij}
\end{bmatrix}
\]
The pairwise $a_{ij}$ and $b_{ij}$ can by quickly found by minimizing the least squares error between the two relationships for time as a function of the first diffusion maps coordinate. We then form the block matrix G, and the top two eigenvectors can be used to form a block eigenvector which contains the scales and shifts from the globally optimal solution. Again, there are degrees of freedom in the scale and in the shift, so we apply a final scale and shift such that the mean shift that we apply to the data is 0 and the mean log scale we apply is also 0. This is in an effort to modify the data as little as possible, and also to make the algorithm deterministic.

\subsection{Data fusion to construct a representative trajectory}

\section{Systems of Interest for Future Work}



\section{Conclusions}

\bibliographystyle{abbrv}
\bibliography{../first-prop}
\end{document}